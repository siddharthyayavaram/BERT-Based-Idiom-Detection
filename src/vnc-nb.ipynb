{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "import ast\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load list from the pkl file\n",
    "with open('vnc.pkl', 'rb') as f:\n",
    "    vnc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'corpus': 'VNC-skewed',\n",
       " 'split': '',\n",
       " 'pie_type': 'blow smoke',\n",
       " 'sense_label': 'L',\n",
       " 'binary_label': 'l',\n",
       " 'predicted_label': '',\n",
       " 'classification': '',\n",
       " 'context': ['<w c5=\"AV0\" hw=\"anyway\" pos=\"ADV\">Anyway <c c5=\"PUN\">, <w c5=\"NP0\" hw=\"jack\" pos=\"SUBST\">Jack <w c5=\"AT0\" hw=\"the\" pos=\"ART\">The <w c5=\"NN1\" hw=\"lad\" pos=\"SUBST\">Lad <w c5=\"VVD\" hw=\"say\" pos=\"VERB\">said <c c5=\"PUN\">, <w c5=\"VVG\" hw=\"rehearse\" pos=\"VERB\">rehearsing <c c5=\"PUN\">, <w c5=\"CJC\" hw=\"and\" pos=\"CONJ\">and <w c5=\"VVD\" hw=\"blow\" pos=\"VERB\">blew <w c5=\"NN1-VVB\" hw=\"smoke\" pos=\"SUBST\">smoke <w c5=\"PRP\" hw=\"in\" pos=\"PREP\">in <w c5=\"DPS\" hw=\"he\" pos=\"PRON\">his <w c5=\"NN1\" hw=\"face\" pos=\"SUBST\">face <c c5=\"PUN\">.'],\n",
       " 'context_untokenized': ['<w c5=\"AV0\" hw=\"anyway\" pos=\"ADV\">Anyway<c c5=\"PUN\">, <w c5=\"NP0\" hw=\"jack\" pos=\"SUBST\">Jack <w c5=\"AT0\" hw=\"the\" pos=\"ART\">The <w c5=\"NN1\" hw=\"lad\" pos=\"SUBST\">Lad <w c5=\"VVD\" hw=\"say\" pos=\"VERB\">said<c c5=\"PUN\">, <w c5=\"VVG\" hw=\"rehearse\" pos=\"VERB\">rehearsing<c c5=\"PUN\">, <w c5=\"CJC\" hw=\"and\" pos=\"CONJ\">and <w c5=\"VVD\" hw=\"blow\" pos=\"VERB\">blew <w c5=\"NN1-VVB\" hw=\"smoke\" pos=\"SUBST\">smoke <w c5=\"PRP\" hw=\"in\" pos=\"PREP\">in <w c5=\"DPS\" hw=\"he\" pos=\"PRON\">his <w c5=\"NN1\" hw=\"face\" pos=\"SUBST\">face<c c5=\"PUN\">.'],\n",
       " 'offsets': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = ast.literal_eval(str(vnc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idioms = [ast.literal_eval(str(vnc[i]))['pie_type'] for i in range(len(vnc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1 = []\n",
    "\n",
    "for i in range(len(vnc)):\n",
    "    soup = BeautifulSoup(vnc[i].context_untokenized[0], 'html.parser')\n",
    "    sentences = soup.find_all('w')\n",
    "    # sentence_text = ''.join(word.text for word in sentences[0])\n",
    "    l = [word.text for word in sentences]\n",
    "    sentence_text = max(l, key=len)\n",
    "    sen1.append(sentence_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Take heart!'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(min(sen1, key = len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_characters = [',', '.', '‘', '’', '!', '?', ';', ':', '\"', \"'\", '-', '—','_', '*', '@', '#', '$', '%', '^', '&', '(', ')', '[', ']', '{', '}', '<', '>', '/', '\\\\', '|', '~', '`', '+', '=', '§', '±', '¢', '£', '¤', '¦', '¨', '©', 'ª', '¬', '¯', '°', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', '×', '÷', '¢', '¥', '£', '€', '¤']\n",
    "special_char_counts = {char: 0 for char in special_characters}\n",
    "\n",
    "# Iterate through each sentence in sen1\n",
    "for sentence in sen1:\n",
    "    # Iterate through each character in the sentence\n",
    "    for char in sentence:\n",
    "        # Increment the count of the character if it's in the special characters list\n",
    "        if char in special_characters:\n",
    "            special_char_counts[char] += 1\n",
    "\n",
    "# Print the counts\n",
    "for char, count in special_char_counts.items():\n",
    "    print(f\"Count of '{char}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = []\n",
    "\n",
    "for i in sen1:\n",
    "    if ',' in i:\n",
    "        index = i.index(',')\n",
    "        if index < len(i) - 1 and (i[index + 1].isalpha() or i[index + 1].isdigit()):\n",
    "            ct.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"After initially arousing astonishment,The Magic Flute caught the public's imagination.\",\n",
       " \"But you don't have to spend £1,000 on Versace or cut a threatening figure in studs and straps to keep abreast of the trend.\",\n",
       " \"Recently you may have had the misfortune to sit through John Hughes' latest slice of suburban comedy,Uncle Buck .\",\n",
       " \"But Keegan is ready to pull the plug on a £500,000 deal for Mechelen's Swedish striker Kennet Andersson.\",\n",
       " 'The cost is $10,000-70,000 — though customers must provide their own cars (which need to have engines powerful enough to pull all that extra weight).',\n",
       " 'Responding to criticism that Germany was not pulling its weight within the alliance the government announced on Jan. 25 that its financial contribution to the Gulf effort totalled DM5,300 million, of which DM3,400 million was for Western allies and DM1,900 million for \"frontline\" Arab states (US$1.00=DM1.4885 as at Jan. 28, 1991).',\n",
       " \"So we've discovered quite a lot about it all really but erm in the second stanza he says,through the window I see no star.\",\n",
       " 'A POLICEMAN was slashed with a knife when he tried to arrest a man who had just set fire to a £9,000 Volvo car, Liverpool Crown Court heard yesterday.',\n",
       " 'A POLICEMAN was slashed with a knife when he tried to arrest a man who had just set fire to a £9,000 Volvo car, Liverpool Crown Court heard yesterday.',\n",
       " 'The association has taken heart from a recent prosecution which saw one offender fined £20,000.']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare ,  one realizes ,  had the supreme misfortune not to be an Inkling . \n"
     ]
    }
   ],
   "source": [
    "special_characters = [',', '.', '…','‘', '’', '!', '?', ';', ':', '\"', \"'\", '-', '—','_', '*', '@', '#', '$', '%', '^', '&', '(', ')', '[', ']', '{', '}', '<', '>', '/', '\\\\', '|', '~', '`', '+', '=', '§', '±', '¢', '£', '¤', '¦', '¨', '©', 'ª', '¬', '¯', '°', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', '×', '÷', '¢', '¥', '£', '€', '¤']\n",
    "\n",
    "def augment_special_characters(input_string):\n",
    "    for char in special_characters:\n",
    "        input_string = input_string.replace(char, ' '+char+' ')\n",
    "    return input_string\n",
    "\n",
    "# Example usage:\n",
    "# input_string = \"Hello! How are you? I'm doing well, thank you.\"\n",
    "input_string = sen1[925]\n",
    "output_string = augment_special_characters(input_string)\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sen1)):\n",
    "    sen1[i] = sen1[i].replace(\"cannot\",'can’t')\n",
    "    sen1[i] = sen1[i].replace(\"gotta\",'got to')\n",
    "    sen1[i] = sen1[i].replace(\"gonna\",'going to')\n",
    "    sen1[i] = sen1[i].replace(\"wont\",\"won't\")\n",
    "\n",
    "for i in range(len(sen1)):\n",
    "    sen1[i] = augment_special_characters(sen1[i])\n",
    "\n",
    "for i in range(len(sen1)):\n",
    "    sen1[i] = ' '.join(sen1[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def lemmatize(word):\n",
    "    return nlp(word)[0].lemma_\n",
    "\n",
    "def tag_consecutive_lemmatized(sentence, phrase):\n",
    "    sentence_words = [lemmatize(word.text) for word in nlp(sentence)]\n",
    "    phrase_words = [lemmatize(word.text) for word in nlp(phrase)]\n",
    "\n",
    "    # print(sentence_words)\n",
    "    # print(phrase_words)\n",
    "\n",
    "    tagged_list = [0] * len(sentence_words)\n",
    "\n",
    "    for i in range(len(sentence_words)):\n",
    "        if sentence_words[i]==phrase_words[0]:\n",
    "            while sentence_words[i]!= phrase_words[-1]:\n",
    "                if sentence_words[i]==phrase_words[0]:\n",
    "                    tagged_list = [0] * len(sentence_words)\n",
    "                    tagged_list[i]=1\n",
    "                    i+=1\n",
    "                else:    \n",
    "                    tagged_list[i]=1\n",
    "                    i+=1\n",
    "                if i ==len(sentence_words):\n",
    "                    tagged_list = [0] * len(sentence_words)\n",
    "                    phrase_words.reverse()\n",
    "\n",
    "                    for j in range(len(sentence_words)):\n",
    "                        if(sentence_words[j] == phrase_words[0]):\n",
    "                            while sentence_words[j] != phrase_words[-1]:\n",
    "                                if sentence_words[j]==phrase_words[0]:\n",
    "                                    tagged_list = [0] * len(sentence_words)\n",
    "                                    tagged_list[j]=1\n",
    "                                    j+=1\n",
    "                                else:    \n",
    "                                    tagged_list[j]=1\n",
    "                                    j+=1\n",
    "                            tagged_list[j] = 1\n",
    "                            return tagged_list\n",
    "            tagged_list[i]=1\n",
    "            break\n",
    "\n",
    "    return tagged_list\n",
    "# Example usage:\n",
    "sentence = \"The Mani Pulite — clean hands — investigation in Italy that has now snared Ing C Olivetti & Co SpA ( see page seven ) has won universal admiration for its thoroughness and determination not to pull punches ; our only concern is that if you simply put the two words Mani and Pulite together , the thing doesn ' t sound nearly so squeaky clean\"\n",
    "target_phrase = \"the\"\n",
    "\n",
    "result = tag_consecutive_lemmatized(sentence, target_phrase)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2981/2981 [08:13<00:00,  6.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open files for writing\n",
    "count = []\n",
    "with open(\"tags_vnc.txt\", \"w\", encoding=\"utf-8\") as tags_file:\n",
    "    # Writing to sentences.txt\n",
    "    with open('words_vnc.txt', 'w', encoding='utf-8') as sentences_file:\n",
    "\n",
    "        for i in tqdm(range(len(sen1[:]))):\n",
    "            labels = tag_consecutive_lemmatized(sen1[i],idioms[i])\n",
    "            # print(sen1[i])\n",
    "\n",
    "            # print(labels)\n",
    "\n",
    "            if (len(sen1[i].split()) == len(labels)):\n",
    "                count.append(i)\n",
    "                tags_file.write(\" \".join(map(str, labels)) + \"\\n\")\n",
    "                sentences_file.write(sen1[i] + '\\n')\n",
    "                \n",
    "tags_file.close()\n",
    "sentences_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    ner_labels = examples[\"ner_labels\"]\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    print(tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids']))\n",
    "    print(word_ids)\n",
    "    print(ner_labels)\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(ner_labels[word_idx])\n",
    "        else:\n",
    "            label_ids.append(ner_labels[word_idx] if label_all_tokens else -100)\n",
    "\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'I', 'can', 'not', 'hold', 'it', 'any', 'long', ',', 'I', 'blow', 'the', 'smoke', 'into', 'his', 'mouth', '.']\n",
      "['blow', 'smoke']\n",
      "17\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0]\n",
      "['When', 'I', 'cannot', 'hold', 'it', 'any', 'longer', ',', 'I', 'blow', 'the', 'smoke', 'into', 'his', 'mouth', '.']\n",
      "['[CLS]', 'when', 'i', 'cannot', 'hold', 'it', 'any', 'longer', ',', 'i', 'blow', 'the', 'smoke', 'into', 'his', 'mouth', '.', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, None]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0]\n",
      "-100                 [CLS]\n",
      "0                 when\n",
      "0                 i\n",
      "0                 cannot\n",
      "0                 hold\n",
      "0                 it\n",
      "0                 any\n",
      "0                 longer\n",
      "0                 ,\n",
      "0                 i\n",
      "0                 blow\n",
      "1                 the\n",
      "1                 smoke\n",
      "1                 into\n",
      "0                 his\n",
      "0                 mouth\n",
      "0                 .\n",
      "-100                 [SEP]\n"
     ]
    }
   ],
   "source": [
    "l = 'When I cannot hold it any longer , I blow the smoke into his mouth .'\n",
    "\n",
    "result = tag_consecutive_lemmatized(l,  'blow smoke')\n",
    "print(len(result))\n",
    "print(result)\n",
    "split_sentence = l.split()\n",
    "print(split_sentence)\n",
    "\n",
    "d = {'ner_labels':result, 'tokens':l.split()}\n",
    "t =tokenize_and_align_labels(d)\n",
    "a = t['labels']\n",
    "b = tokenizer.convert_ids_to_tokens(t['input_ids'])\n",
    "for i,j in zip(a,b):\n",
    "    print(i,j,sep=\"                 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit = []\n",
    "for i in range(len(vnc)):\n",
    "    h = ast.literal_eval(str(vnc[i]))\n",
    "    if(h['sense_label']=='L'):\n",
    "        lit.append(i)\n",
    "\n",
    "unk = []\n",
    "for i in range(len(vnc)):\n",
    "    h = ast.literal_eval(str(vnc[i]))\n",
    "    if(h['sense_label']=='Q'):\n",
    "        unk.append(i)\n",
    "\n",
    "text_data = []\n",
    "tag_data = []\n",
    "\n",
    "with open('words_vnc.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read lines and remove newline characters\n",
    "    text_data = [line.rstrip('\\n').split() for line in file.readlines()]\n",
    "\n",
    "with open('tags_vnc.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read lines and remove newline characters\n",
    "    tag_data = [line.rstrip('\\n').split() for line in file.readlines()]\n",
    "\n",
    "def convert_tags_to_integers_list(list_of_tag_lists):\n",
    "    tag_mapping = {'O': 0, 'B-IDIOM': 1,'I-IDIOM':1}\n",
    "    return [[tag_mapping[tag] for tag in inner_list] for inner_list in list_of_tag_lists]\n",
    "\n",
    "def convert_tags_to_integers_list_1(list_of_tag_lists):\n",
    "    tag_mapping = {'0': 0, '1': 1}\n",
    "    return [[tag_mapping[tag] for tag in inner_list] for inner_list in list_of_tag_lists]\n",
    "\n",
    "tag_data = convert_tags_to_integers_list_1(tag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = []\n",
    "for i in range(len(text_data)):\n",
    "    if len(text_data[i]) != len(tag_data[i]):\n",
    "        ctr.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_list_of_lists(lst, lit):\n",
    "    updated_lst = []\n",
    "    for i, sub_lst in enumerate(lst):\n",
    "        if i in lit:\n",
    "            updated_sub_lst = [0] * len(sub_lst)\n",
    "        else:\n",
    "            updated_sub_lst = sub_lst[:]\n",
    "        updated_lst.append(updated_sub_lst)\n",
    "    return updated_lst\n",
    "\n",
    "# Example usage:\n",
    "list_of_lists = [[1, 2, 3,5,4], [4, 5, 6,6,6,6], [7, 8, 9]]\n",
    "indices_to_update = [0, 2]\n",
    "\n",
    "updated_list_of_lists = update_list_of_lists(list_of_lists, indices_to_update)\n",
    "print(updated_list_of_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_data = update_list_of_lists(tag_data, lit)\n",
    "text_data = [text_data[i] for i in range(len(text_data)) if i not in unk]\n",
    "tag_data = [tag_data[i] for i in range(len(tag_data)) if i not in unk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2568, 2568)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_data),len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open files for writing\n",
    "with open(\"tags_vnc_fix.txt\", \"w\", encoding=\"utf-8\") as tags_file:\n",
    "    # Writing to sentences.txt\n",
    "    with open('words_vnc_fix.txt', 'w', encoding='utf-8') as sentences_file:\n",
    "        \n",
    "        for i in range(len(text_data)):\n",
    "            sentence = ' '.join(text_data[i])\n",
    "            sentences_file.write(sentence + '\\n')\n",
    "\n",
    "            tags_file.write(\" \".join(map(str, tag_data[i])) + \"\\n\")\n",
    "\n",
    "# Close the file\n",
    "tags_file.close()\n",
    "sentences_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idioms_wo_unk = []\n",
    "\n",
    "for i in range(len(idioms)):\n",
    "    if i not in unk:\n",
    "        idioms_wo_unk.append(idioms[i])\n",
    "\n",
    "idioms_wo_lit = []\n",
    "\n",
    "for i in range(len(idioms_wo_unk)):\n",
    "    if i not in lit:\n",
    "        idioms_wo_lit.append(idioms[i])\n",
    "\n",
    "idi_ctt = {}\n",
    "for i in idioms_wo_lit:\n",
    "    if i in idi_ctt:\n",
    "        idi_ctt[i] += 1\n",
    "    else:\n",
    "        idi_ctt[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idi_ct = {}\n",
    "for i in idioms:\n",
    "    if i in idi_ct:\n",
    "        idi_ct[i] += 1\n",
    "    else:\n",
    "        idi_ct[i] = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
